{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-06-28 13:56:44,836 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\a\\AppData\\Local\\Temp\\jieba.cache\n",
      "2018-06-28 13:56:46,927 : DEBUG : Dumping model to file cache C:\\Users\\a\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.316 seconds.\n",
      "2018-06-28 13:56:47,155 : DEBUG : Loading model cost 2.316 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-06-28 13:56:47,158 : DEBUG : Prefix dict has been built succesfully.\n",
      "2018-06-28 13:56:57,586 : INFO : collecting all words and their counts\n",
      "2018-06-28 13:56:57,592 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-28 13:56:58,185 : INFO : collected 34034 word types from a corpus of 669564 raw words and 67 sentences\n",
      "2018-06-28 13:56:58,188 : INFO : Loading a fresh vocabulary\n",
      "2018-06-28 13:56:58,272 : INFO : min_count=5 retains 8020 unique words (23% of original 34034, drops 26014)\n",
      "2018-06-28 13:56:58,274 : INFO : min_count=5 leaves 629398 word corpus (94% of original 669564, drops 40166)\n",
      "2018-06-28 13:56:58,357 : INFO : deleting the raw counts dictionary of 34034 items\n",
      "2018-06-28 13:56:58,364 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2018-06-28 13:56:58,367 : INFO : downsampling leaves estimated 458211 word corpus (72.8% of prior 629398)\n",
      "2018-06-28 13:56:58,369 : INFO : estimated required memory for 8020 words and 300 dimensions: 23258000 bytes\n",
      "2018-06-28 13:56:58,432 : INFO : resetting layer weights\n",
      "2018-06-28 13:56:58,698 : INFO : training model with 3 workers on 8020 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-28 13:56:59,723 : INFO : PROGRESS: at 13.13% examples, 293207 words/s, in_qsize 6, out_qsize 0\n",
      "2018-06-28 13:57:00,726 : INFO : PROGRESS: at 22.39% examples, 253032 words/s, in_qsize 5, out_qsize 0\n",
      "2018-06-28 13:57:01,727 : INFO : PROGRESS: at 36.72% examples, 277901 words/s, in_qsize 5, out_qsize 1\n",
      "2018-06-28 13:57:02,744 : INFO : PROGRESS: at 51.34% examples, 290642 words/s, in_qsize 6, out_qsize 0\n",
      "2018-06-28 13:57:03,759 : INFO : PROGRESS: at 64.78% examples, 293202 words/s, in_qsize 5, out_qsize 1\n",
      "2018-06-28 13:57:04,788 : INFO : PROGRESS: at 81.49% examples, 306778 words/s, in_qsize 6, out_qsize 0\n",
      "2018-06-28 13:57:05,802 : INFO : PROGRESS: at 93.43% examples, 301300 words/s, in_qsize 6, out_qsize 0\n",
      "2018-06-28 13:57:06,220 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-28 13:57:06,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-28 13:57:06,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-28 13:57:06,271 : INFO : training on 3347820 raw words (2291445 effective words) took 7.6s, 302758 effective words/s\n",
      "2018-06-28 13:57:06,274 : INFO : saving Word2Vec object under C:/Users/a/Desktop/Python/data/Word300.model, separately None\n",
      "2018-06-28 13:57:06,278 : INFO : not storing attribute syn0norm\n",
      "2018-06-28 13:57:06,286 : INFO : not storing attribute cum_table\n",
      "2018-06-28 13:57:06,844 : INFO : saved C:/Users/a/Desktop/Python/data/Word300.model\n",
      "2018-06-28 13:57:06,857 : INFO : loading Word2Vec object from C:/Users/a/Desktop/Python/data/Word300.model\n",
      "2018-06-28 13:57:07,217 : INFO : loading wv recursively from C:/Users/a/Desktop/Python/data/Word300.model.wv.* with mmap=None\n",
      "2018-06-28 13:57:07,219 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-06-28 13:57:07,221 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-28 13:57:07,224 : INFO : loaded C:/Users/a/Desktop/Python/data/Word300.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test tensor: (102024, 150)\n",
      "Total 30000 word vectors.\n",
      "x_tra shape  (17000, 150)\n",
      "X_val shape  (3000, 150)\n",
      "y_tra shape  (17000,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 300)          721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 300)               450       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,798,963\n",
      "Trainable params: 798,963\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "simple_lstm_w2v_vectors_0.25_0.25\n",
      "Train on 17000 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "17000/17000 [==============================] - 711s 42ms/step - loss: 0.6449 - mean_squared_error: 0.6449 - val_loss: 0.4796 - val_mean_squared_error: 0.4796\n",
      "Epoch 2/20\n",
      "17000/17000 [==============================] - 711s 42ms/step - loss: 0.4723 - mean_squared_error: 0.4723 - val_loss: 0.4096 - val_mean_squared_error: 0.4096\n",
      "Epoch 3/20\n",
      "17000/17000 [==============================] - 676s 40ms/step - loss: 0.4416 - mean_squared_error: 0.4416 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "Epoch 4/20\n",
      "17000/17000 [==============================] - 622s 37ms/step - loss: 0.4220 - mean_squared_error: 0.4220 - val_loss: 0.4000 - val_mean_squared_error: 0.4000\n",
      "Epoch 5/20\n",
      "17000/17000 [==============================] - 667s 39ms/step - loss: 0.4106 - mean_squared_error: 0.4106 - val_loss: 0.3509 - val_mean_squared_error: 0.3509\n",
      "Epoch 6/20\n",
      "17000/17000 [==============================] - 641s 38ms/step - loss: 0.3973 - mean_squared_error: 0.3973 - val_loss: 0.3674 - val_mean_squared_error: 0.3674\n",
      "Epoch 7/20\n",
      "17000/17000 [==============================] - 670s 39ms/step - loss: 0.3902 - mean_squared_error: 0.3902 - val_loss: 0.3472 - val_mean_squared_error: 0.3472\n",
      "Epoch 8/20\n",
      "17000/17000 [==============================] - 660s 39ms/step - loss: 0.3794 - mean_squared_error: 0.3794 - val_loss: 0.3448 - val_mean_squared_error: 0.3448\n",
      "Epoch 9/20\n",
      "17000/17000 [==============================] - 677s 40ms/step - loss: 0.3731 - mean_squared_error: 0.3731 - val_loss: 0.3442 - val_mean_squared_error: 0.3442\n",
      "Epoch 10/20\n",
      "17000/17000 [==============================] - 666s 39ms/step - loss: 0.3600 - mean_squared_error: 0.3600 - val_loss: 0.3417 - val_mean_squared_error: 0.3417\n",
      "Epoch 11/20\n",
      "17000/17000 [==============================] - 659s 39ms/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3484 - val_mean_squared_error: 0.3484\n",
      "Epoch 12/20\n",
      "17000/17000 [==============================] - 662s 39ms/step - loss: 0.3429 - mean_squared_error: 0.3429 - val_loss: 0.3476 - val_mean_squared_error: 0.3476\n",
      "Start making the submission before fine-tuning\n",
      "102024/102024 [==============================] - 1252s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import jieba\n",
    "import pickle\n",
    "import codecs\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import pandas as pd \n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from string import punctuation\n",
    "from gensim.models import word2vec\n",
    "from keras.models import Sequential\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "\n",
    "###########################\n",
    "#####Attention类定义#######\n",
    "###########################\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "##########################\n",
    "##########函数集合########\n",
    "##########################\n",
    "\n",
    "###结巴分词\n",
    "def cuttxt(train_file_name):\n",
    "    f1 = open(cut_txt,encoding=\"utf-8\")\n",
    "    f2= open(cut_txt_r,\"w\",encoding=\"utf-8\")\n",
    "    lines = f1.readlines()\n",
    "    for line in lines:\n",
    "        line.replace('\\t',\"\").replace('\\n',\"\").replace(\" \",\"\")\n",
    "        seg_list=jieba.cut(line,cut_all=False)\n",
    "        f2.write(\" \".join(seg_list))\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    return cut_txt_r\n",
    "\n",
    "###word2vec模型训练\n",
    "def model_train(train_file_name, save_model_file,n_dim):  # model_file_name为训练语料的路径,save_model为保存模型名\n",
    "    # 模型训练，生成词向量\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    train_file_name_cut=cuttxt(train_file_name)\n",
    "    sentences = word2vec.Text8Corpus(train_file_name_cut)  # 加载语料\n",
    "    model = gensim.models.Word2Vec(sentences, size=n_dim)  # 训练skip-gram模型; 默认window=5\n",
    "    model.save(save_model_file)\n",
    "    \n",
    "###定义模型\n",
    "def get_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(inp)\n",
    "    x = lstm_layer(embedded_sequences)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    merged = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    outp = Dense(1)(merged)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "#########################   \n",
    "#######命名预处理########\n",
    "#########################\n",
    "\n",
    "###命名文件——训练集与测试集\n",
    "train_name = \"./training-inspur.csv\"\n",
    "predict_name = \"./Preliminary-texting.csv\"\n",
    "\n",
    "###cut_txt——分词前文本集合\n",
    "###cut_txt_r——分词后文本词集合\n",
    "###文件编码格式为utf-8\n",
    "cut_txt = './dl_text.txt'\n",
    "cut_txt_r = './dl_text_r.txt' \n",
    "\n",
    "###词向量模型以及特征保存路径\n",
    "save_model_name = './Word300.model'\n",
    "save_feature = './df_fea.pkl'\n",
    "\n",
    "###########################\n",
    "######实验参数设置#########\n",
    "###########################\n",
    "\n",
    "###w2v的特征维度\n",
    "max_features = 30000\n",
    "maxlen = 200\n",
    "validation_split = 0.1\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 30000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "###向量维度\n",
    "embed_size = 300\n",
    "\n",
    "###LSTM参数\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.25\n",
    "rate_drop_dense = 0.25\n",
    "\n",
    "act = 'relu'\n",
    "\n",
    "###########################\n",
    "########读取数据###########\n",
    "###########################\n",
    "train = pd.read_csv(train_name)\n",
    "train_row = train.shape[0]\n",
    "df_train = train\n",
    "predict = pd.read_csv(predict_name)\n",
    "df_test = predict\n",
    "\n",
    "\n",
    "###########################\n",
    "#########数据预处理########\n",
    "###########################\n",
    "###判断w2v模型是否存在\n",
    "if not os.path.exists(save_model_name):\n",
    "    model_train(cut_txt_r, save_model_name,EMBEDDING_DIM)\n",
    "else:\n",
    "    print('此训练模型已经存在，不用再次训练')\n",
    "\n",
    "###加载w2v模型\n",
    "w2v_model = word2vec.Word2Vec.load(save_model_name)\n",
    "\n",
    "###判断特征是否存在：Y直接加载 N结巴分词生成特征\n",
    "if not os.path.exists(save_feature):\n",
    "    df = pd.concat([df_train,df_test])\n",
    "    df['cut_discuss'] = df['COMMCONTENT'].astype(\"str\").map(lambda x : \" \".join(i for i in jieba.cut(x)))\n",
    "    fw = open(save_feature,'wb') \n",
    "    pickle.dump(df,fw)\n",
    "    fw.close()\n",
    "else:\n",
    "    print(\"特征存在，直接加载...\")\n",
    "    fw = open(save_feature,'rb') \n",
    "    df = pickle.load(fw)\n",
    "    fw.close()\n",
    "\n",
    "###取出第三列的所有行\n",
    "X_train = df.iloc[:train_row,3]\n",
    "X_test = df.iloc[train_row:,3]\n",
    "\n",
    "###Tokenizer进行词法分析\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "\n",
    "###转换word下标的向量形式\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "###将序列填充到maxlen长度\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "###训练集结果集\n",
    "y =df_train['COMMLEVEL'].values\n",
    "\n",
    "\n",
    "###找出lstm权重\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "###计算权重\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: \n",
    "        continue\n",
    "    else :\n",
    "        try:\n",
    "            embedding_vector = w2v_model[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "########划分训练集#########\n",
    "###########################\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y, train_size=0.85, random_state=250)\n",
    "\n",
    "###########################\n",
    "#########定义模型##########\n",
    "###########################\n",
    "\n",
    "####模型参数\n",
    "model = get_model()\n",
    "STAMP = 'simple_lstm_w2v_vectors_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)\n",
    "\n",
    "###提前停止定义\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=2)\n",
    "###保存位置\n",
    "bst_model_path = \"./\" + STAMP + '.h5'\n",
    "###保存最优\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "###批尺寸64\n",
    "batch_size = 64\n",
    "###迭代次数20\n",
    "epochs = 20\n",
    "\n",
    "#########################\n",
    "########模型拟合#########\n",
    "#########################\n",
    "hist = model.fit(X_tra, y_tra,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "         callbacks=[early_stopping,model_checkpoint])\n",
    "         \n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "#########################\n",
    "#########模型预测########\n",
    "#########################\n",
    "y_test = model.predict(test, batch_size=256, verbose=1)\n",
    "\n",
    "#########################\n",
    "##########结果保存#######\n",
    "#########################\n",
    "f = codecs.open(\"./result.txt\",'w','utf-8')\n",
    "for i in y_test:\n",
    "    f.write(str(i).strip('[]')+'\\r\\n')#\\r\\n为换行符\n",
    "f.close()\n",
    "\n",
    "b = np.loadtxt('./result.txt')\n",
    "df=pd.DataFrame(b, columns = ['COMMLEVEL'])\n",
    "#参数可调优\n",
    "df[\"y\"]=df['COMMLEVEL'].apply(lambda x: 1 if x<1.758 else(3 if x>2.5726 else 2))\n",
    "#输出结果集\n",
    "df.to_csv('./dsjyycxds_preliminary.txt',columns=['y'],index = False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
